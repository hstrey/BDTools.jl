{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a1e4eee-17d1-4900-af0e-38960fe8ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/programming/BDTools.jl/examples`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413534f9-7a86-40fa-86ce-abd065887c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Documents/programming/BDTools.jl/examples/Project.toml`\n",
      "\u001b[33m⌅\u001b[39m \u001b[90m[3bd65402] \u001b[39m\u001b[92m+ Optimisers v0.3.4\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Documents/programming/BDTools.jl/examples/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "Pkg.add([\"Plots\",\"Lux\",\"ADTypes\",\"Optimisers\",\"MLUtils\",\"Zygote\",\"HDF5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ee800d-7d62-4b21-8e94-7452636b0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "using HDF5: HDF5\n",
    "using Plots, Random, Lux, ADTypes, Optimisers, Zygote, Printf, MLUtils, Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29751527-1707-40c3-a2a4-5f53cd1cc635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deserialize (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct GroundTruthCleaned\n",
    "    data::Array{Float64,3}\n",
    "end\n",
    "    \n",
    "function deserialize(filepath::String, ::Type{GroundTruthCleaned})\n",
    "    HDF5.h5open(filepath, \"r\") do io\n",
    "        GroundTruthCleaned(\n",
    "            io[\"GroundTruthCleaned/data\"] |> read\n",
    "        )\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34504639-6f74-441d-a9e1-2940e1c3e855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "standardize"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    standardize(data::AbstractArray; dims=1)\n",
    "\n",
    "Return standardized `data` over dimension `dims`.\n",
    "\"\"\"\n",
    "function standardize(data::AbstractArray; dims=1)\n",
    "    μ = mean(data, dims=dims)\n",
    "    σ = std(data, dims=dims, mean=μ)\n",
    "    (data.-μ) ./ σ, μ, σ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02eafbd1-4a47-46cb-9715-8b6b790ee510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400×1×1056 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       " -0.25204962\n",
       "  0.8046118\n",
       " -1.1035193\n",
       " -0.79414064\n",
       "  1.9682941\n",
       "  0.5637778\n",
       " -0.012230185\n",
       " -0.21458794\n",
       " -0.07579102\n",
       "  1.606328\n",
       " -0.5129932\n",
       " -0.49603918\n",
       "  1.8115286\n",
       "  ⋮\n",
       "  0.60853773\n",
       " -0.38985053\n",
       " -0.50285214\n",
       " -0.4290375\n",
       " -0.2008933\n",
       "  0.90451586\n",
       " -0.26445413\n",
       "  0.938424\n",
       " -1.2621317\n",
       "  1.3216174\n",
       "  1.6147438\n",
       "  1.8672619\n",
       "\n",
       "[:, :, 2] =\n",
       "  0.46098885\n",
       "  0.7983072\n",
       "  0.9000875\n",
       " -0.5965039\n",
       "  1.0699972\n",
       " -0.62849957\n",
       " -1.6455749\n",
       " -0.9128678\n",
       " -0.49982587\n",
       "  1.4695151\n",
       " -1.2132338\n",
       "  1.2019722\n",
       " -0.807777\n",
       "  ⋮\n",
       "  0.23529959\n",
       "  0.5053252\n",
       " -0.57064825\n",
       " -0.05665371\n",
       "  0.18813378\n",
       " -1.2495686\n",
       "  0.4842252\n",
       " -0.3646048\n",
       "  0.07366619\n",
       "  2.0850706\n",
       "  0.35293218\n",
       "  0.8921558\n",
       "\n",
       "[:, :, 3] =\n",
       "  0.2637831\n",
       " -0.953591\n",
       "  1.8452263\n",
       "  0.82109296\n",
       "  0.5363555\n",
       " -0.87426835\n",
       " -0.23479255\n",
       " -2.2671652\n",
       "  0.2375749\n",
       "  0.2216988\n",
       " -0.96207213\n",
       " -1.3896456\n",
       "  0.358906\n",
       "  ⋮\n",
       " -0.8054758\n",
       " -0.74574023\n",
       "  0.6919415\n",
       " -0.61785483\n",
       " -1.5915744\n",
       "  1.1232259\n",
       "  0.41837677\n",
       "  0.8646118\n",
       " -0.68044275\n",
       "  0.41274798\n",
       " -1.5104008\n",
       " -0.96333826\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 1054] =\n",
       "  1.2556438\n",
       "  0.47701725\n",
       "  0.78989226\n",
       "  0.09632059\n",
       " -0.17908627\n",
       " -0.6883796\n",
       " -1.5662296\n",
       "  0.029498652\n",
       " -0.27425256\n",
       " -1.1379267\n",
       " -2.2213187\n",
       " -0.97997147\n",
       "  0.09836125\n",
       "  ⋮\n",
       "  0.061560303\n",
       "  0.23267712\n",
       " -1.183832\n",
       " -0.35356888\n",
       "  0.71058804\n",
       " -1.5209943\n",
       "  0.3511478\n",
       "  0.8979092\n",
       " -0.56112015\n",
       "  1.1054798\n",
       "  0.09296688\n",
       "  0.6184645\n",
       "\n",
       "[:, :, 1055] =\n",
       " -0.7809072\n",
       " -0.48130742\n",
       "  1.3145847\n",
       "  0.4841204\n",
       " -0.37250888\n",
       " -0.69025487\n",
       " -0.4534358\n",
       " -0.38403565\n",
       " -2.0515893\n",
       " -1.4380802\n",
       " -1.0181497\n",
       " -0.9068922\n",
       " -1.4391453\n",
       "  ⋮\n",
       "  0.840844\n",
       "  0.6434233\n",
       " -0.8567113\n",
       " -0.6565188\n",
       "  0.4330819\n",
       " -1.2658566\n",
       "  0.37307832\n",
       " -0.69281656\n",
       " -0.24149568\n",
       "  1.5125444\n",
       "  0.9070435\n",
       "  0.2701576\n",
       "\n",
       "[:, :, 1056] =\n",
       " -0.53318334\n",
       " -0.1288179\n",
       "  0.3041636\n",
       "  0.48432365\n",
       "  1.2321413\n",
       "  0.6872306\n",
       "  0.80538094\n",
       "  2.3593647\n",
       "  0.25936383\n",
       "  1.0262622\n",
       "  1.7740798\n",
       "  1.1242254\n",
       " -0.35088074\n",
       "  ⋮\n",
       " -2.536639\n",
       "  0.20512928\n",
       "  1.1008247\n",
       "  0.28877953\n",
       " -0.7474611\n",
       "  0.80175\n",
       " -0.12477428\n",
       " -0.6219833\n",
       " -1.0571826\n",
       " -0.8960982\n",
       " -0.3915617\n",
       " -1.532751"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = deserialize(\"gt_clean.h5\", GroundTruthCleaned)\n",
    "ori64, ori_mean, ori_std = standardize(reshape(gt.data[:,:,1],\n",
    "    (size(gt.data[:,:,1])[1],1,size(gt.data[:,:,1])[2])))\n",
    "sim64, sim_mean, sim_std = standardize(reshape(gt.data[:,:,2],\n",
    "    (size(gt.data[:,:,2])[1],1,size(gt.data[:,:,2])[2])))\n",
    "ori = Float32.(ori64)\n",
    "sim = Float32.(sim64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caca1793-803f-42ed-98e0-2fb79133f447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27-element DataLoader(::Tuple{Array{Float32, 3}, Array{Float32, 3}}, shuffle=true, batchsize=8)\n",
       "  with first element:\n",
       "  (400×1×8 Array{Float32, 3}, 400×1×8 Array{Float32, 3},)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(i_train, o_train), (i_test, o_test) = splitobs((ori, sim); at=0.8)\n",
    "train_dataloader = DataLoader(collect.((i_train, o_train)); batchsize=8, shuffle=true)\n",
    "test_dataloader = DataLoader(collect.((i_test, o_test)); batchsize=8, shuffle=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef336b1-6509-4343-8ff7-4a4f934b2d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MersenneTwister(12345)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = MersenneTwister()\n",
    "Random.seed!(rng, 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dfa2dca-4762-4e72-8a05-4c1b815aa0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negr2 (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function negr2(y_pred, y_true)\n",
    "    SS_tot = sum(abs2, vec(y_true) .- mean(vec(y_true)))\n",
    "    return negr2(y_pred, y_true, SS_tot)\n",
    "end\n",
    "\n",
    "function negr2(y_pred, y_true, SS_tot)\n",
    "    SS_res =  sum(abs2, vec(y_true) .- vec(y_pred))\n",
    "    r2 = 1 - SS_res/(SS_tot + eps())\n",
    "    return -r2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a6dad1c-4c30-45ea-8dba-8ccb158ad6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::GenericLossFunction{typeof(Lux.LossFunctionImpl.l2_distance_loss), typeof(mean)}) (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_negr2 = GenericLossFunction(negr2)\n",
    "const mseloss_function = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78840c3f-bd30-4f99-a4c8-61eb73282be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106-element Vector{Tuple{Array{Float32, 3}, Array{Float32, 3}}}:\n",
       " ([-0.12904055; -0.74002886; … ; -0.15248832; -0.9678636;;; -0.08969547; -0.64527464; … ; -0.23183869; -0.94188476;;; -0.26199153; -0.8739501; … ; -0.14498748; -1.0559232;;; -0.1603327; -0.68400973; … ; -0.3176369; -1.0316557;;; 0.54901; 0.6879873; … ; 0.8415705; 1.31415;;; 0.32267785; 0.85443187; … ; 0.2715317; 1.1193714;;; 0.12830567; 0.65194654; … ; 0.29636148; 0.9916155;;; -0.5439662; -0.7517162; … ; -0.7912847; -1.3204832], [-0.2935141; -1.025746; … ; -0.19840579; -1.2898817;;; 0.503593; -0.51018155; … ; 0.7853488; 0.65165615;;; -0.7109996; -0.6936571; … ; 0.85206914; -0.861804;;; 0.40158135; -0.19193839; … ; 1.1738425; 0.033302955;;; 0.3613174; 0.83169854; … ; -0.76082784; 1.219694;;; -0.22638912; -0.13464166; … ; -1.4221513; -1.751251;;; 0.7480971; -0.06747933; … ; -0.42858213; 0.49713326;;; 0.72072005; -0.15104951; … ; 2.207723; 1.0886638])\n",
       " ([-0.73054016; -1.0894914; … ; -0.6148413; -1.4776556;;; -0.16003248; -0.6320133; … ; -0.32371223; -0.9813817;;; 0.23368426; 0.68187183; … ; 0.45775703; 1.1150938;;; 0.09374169; -0.38461572; … ; -0.23191647; -0.71287084;;; -0.25408795; -0.87152326; … ; -0.24144468; -1.1343918;;; 0.21493363; 0.75215757; … ; 0.3328425; 1.0970143;;; 0.25172645; 0.68206257; … ; 0.4900752; 1.1340928;;; -0.09067591; 0.7377767; … ; -0.4159317; 0.54626495], [-0.28476405; 0.14292027; … ; 0.7121284; 0.3174144;;; 0.7240266; -2.4099739; … ; -0.0796899; 0.7642077;;; -0.60478264; 0.84289545; … ; 1.5726453; -0.1893895;;; -0.009983337; -0.8800533; … ; 1.2685115; 0.56251824;;; -0.632486; -1.1345794; … ; 0.0914063; -1.0123966;;; -0.24778774; 0.044821985; … ; 1.0620074; 0.4719065;;; -0.41496062; -0.083839744; … ; 0.675758; 1.6065712;;; -0.5532314; -0.42183653; … ; -0.7588033; 0.16334221])\n",
       " ([0.2778328; 0.611965; … ; 0.59986764; 1.1236482;;; -0.2021239; -0.69403946; … ; -0.38366032; -1.0807226;;; 0.4693723; 0.84887546; … ; 0.523226; 1.2528404;;; 0.1389785; 0.75937897; … ; 0.06477517; 0.91292834;;; 0.53530043; -0.05955976; … ; 0.98333615; 0.5382735;;; 0.24492745; 0.6900422; … ; 0.4684935; 1.1281966;;; 0.40527233; 0.6772599; … ; 0.7137738; 1.2410201;;; 0.4896214; 0.64987713; … ; 0.80957556; 1.2696536], [-0.35856453; 0.24490435; … ; -0.11725327; 1.3809083;;; 0.5449965; -0.44777972; … ; -0.083938845; -0.49228874;;; -0.605985; -0.2903696; … ; 0.3964113; 0.360307;;; -0.60874176; -0.9665339; … ; 0.9028102; 0.39817643;;; -1.0205803; 0.890899; … ; 0.80859494; 0.5599381;;; 1.2297665; 1.7833889; … ; 0.27435988; 1.3867298;;; 1.4610479; 0.67130065; … ; 1.6211982; 0.72161394;;; 0.6790674; 0.3368867; … ; 0.38203204; 1.2336564])\n",
       " ([0.20449963; 0.6757975; … ; 0.41444975; 1.0845515;;; 0.3778741; 0.69654095; … ; 0.66238546; 1.2382786;;; 0.106741905; 0.7001337; … ; 0.16296633; 0.9424282;;; -0.58515054; -0.049193386; … ; -0.97146827; -0.6266664;;; 0.7009517; -0.24123007; … ; 0.93625826; 0.27877894;;; -0.275199; -0.6696053; … ; -0.54332715; -1.1537099;;; -0.8448614; -0.78482294; … ; -0.9842901; -1.3980032;;; 0.4235018; 0.7164709; … ; 0.711812; 1.2617899], [0.04131207; -0.20097397; … ; 0.5760971; 0.848424;;; 2.4826133; 0.39002508; … ; -0.52146304; 0.3480049;;; -1.3056996; 0.5419155; … ; 0.9443083; 0.89212424;;; 2.1488976; 0.18253198; … ; -0.4979279; 2.4851024;;; -0.19297688; -0.30182266; … ; -0.64826185; -1.0795864;;; -1.8757634; 0.3550341; … ; 1.3242332; -1.0585126;;; 0.0123424325; -0.1777885; … ; -1.2410135; -0.60226786;;; 0.123065196; -1.4540149; … ; 0.8217362; 1.3447704])\n",
       " ([0.63141197; 0.5077966; … ; 1.0445154; 1.2308944;;; -0.36212617; -0.81132233; … ; -0.3967389; -1.1433477;;; -0.4063642; -0.67191875; … ; -0.7174342; -1.2385356;;; 0.27127376; 0.9021275; … ; -0.05679506; 0.9234447;;; -0.60596657; 0.17814758; … ; -1.0116094; -0.39337167;;; 0.27236852; 0.6059024; … ; 0.60631657; 1.133701;;; -0.4518678; -0.6412036; … ; -0.79970413; -1.2501487;;; 0.6027603; 1.2035667; … ; 0.23746102; 1.346316], [-0.35052553; -0.9434037; … ; 1.2781899; 1.0668383;;; -1.5236955; -0.5798018; … ; -2.4758754; 0.98704153;;; -1.1795118; -1.0436453; … ; -0.14612167; -2.2454197;;; 0.60465574; 0.03597327; … ; -1.419425; 1.6448351;;; 0.6654049; 0.89507365; … ; -0.9003613; -0.00015169477;;; 1.026958; 0.7220973; … ; 1.1896318; 0.6981548;;; 0.62822664; -0.011683736; … ; -1.2137324; -0.19123967;;; 0.15073709; -0.034877807; … ; -0.9085156; 1.2447166])\n",
       " ([0.20626257; 0.6869327; … ; 0.4042666; 1.0877128;;; 0.8061516; 1.067631; … ; 0.7115479; 1.5061834;;; 0.15135868; 0.69890016; … ; 0.32629508; 1.0335834;;; -0.419203; -0.6207192; … ; -0.78085333; -1.2281678;;; -0.1975171; -0.62288624; … ; -0.46470946; -1.0703098;;; -0.14160396; -0.7272658; … ; -0.2120688; -0.99847466;;; 0.1625369; 0.71607584; … ; 0.28102738; 1.0356973;;; -0.78358257; -0.7832887; … ; -0.96947575; -1.4171517], [-0.30883232; 1.420306; … ; 0.2365354; -0.2238495;;; -1.205079; 0.556605; … ; 1.4566548; 1.9349988;;; 0.32273886; -1.4029299; … ; -1.7830036; -0.7866853;;; 1.3919479; 0.7473576; … ; -0.4862548; 0.37501994;;; 0.110346414; -0.6666011; … ; -1.1950785; -0.7973649;;; 0.2710215; -0.08219421; … ; -0.47949412; -1.1238352;;; 1.0147928; 1.2944084; … ; -0.18107846; 0.03286993;;; 1.2175989; 1.0760045; … ; 1.1823686; 0.8447562])\n",
       " ([0.6966762; 0.3785864; … ; 1.1211616; 1.1088952;;; 0.61359996; 0.037214026; … ; 1.1026995; 0.71225184;;; -0.34523946; -0.7980562; … ; -0.49510178; -1.2305098;;; -0.28086856; -0.6691971; … ; -0.5527819; -1.1585573;;; 0.21117753; 0.705443; … ; 0.40078434; 1.0918238;;; 0.3523923; 0.74521106; … ; 0.58134943; 1.2364178;;; 0.7908816; 0.9106141; … ; 0.84234154; 1.4519557;;; -0.31796822; 0.46556365; … ; -0.7320941; 0.056075316], [-0.46218422; -0.76769817; … ; 0.046120994; -0.6588707;;; -1.3703657; 0.18426846; … ; 0.6088338; 0.7483717;;; 1.244415; -1.2897494; … ; 0.5535076; 1.166384;;; -0.05413086; -0.79357165; … ; 0.019251144; -1.5623205;;; -0.34846705; 0.4534461; … ; 1.4915329; 0.6570742;;; 0.5478991; 0.32625788; … ; 0.059285704; 0.7844157;;; 1.021872; 0.44160587; … ; 1.1773173; 0.53244007;;; -0.7959345; -0.803447; … ; -0.91162735; 0.09260216])\n",
       " ([0.16890013; 0.7091806; … ; 0.29455876; 1.0371491;;; -0.5235118; -0.8605513; … ; -0.69186413; -1.3809608;;; 0.33615294; 0.7229684; … ; 0.57839143; 1.2164328;;; 0.30674514; 0.7457295; … ; 0.50260204; 1.1930833;;; 0.121923715; 0.79587877; … ; 0.075756334; 0.95169127;;; -0.49130666; -0.64447033; … ; -0.8339395; -1.2693981;;; -0.504936; -0.89579827; … ; -0.6265052; -1.3762312;;; 0.5366707; 0.53069514; … ; 0.6855157; 1.0290858], [0.41876835; -0.735247; … ; 0.5690599; 0.60181534;;; -0.97362953; 0.2132043; … ; -0.66745013; -1.2706393;;; -0.5423553; 0.7659889; … ; 0.49326092; 1.1382445;;; 0.1764371; -0.25614056; … ; 0.30701074; 1.3601793;;; -1.0142864; -0.864035; … ; 1.1895242; -0.37571707;;; -0.9937293; -0.77262515; … ; -1.7240824; -0.60972726;;; 2.0587664; -0.16527648; … ; -1.4392707; -1.0347826;;; 0.028209938; -0.5791887; … ; -1.062177; -1.0425918])\n",
       " ([-0.03787348; 0.6920214; … ; -0.29554087; 0.5936182;;; 0.4087583; 0.7465398; … ; 0.64453304; 1.2692633;;; 0.17100036; 0.6824197; … ; 0.3441919; 1.0473126;;; -0.24967909; -0.6945732; … ; -0.46966654; -1.1325381;;; 0.25964448; 0.67533004; … ; 0.51066875; 1.1401885;;; -0.5371103; 0.058809705; … ; -0.9763937; -0.53454345;;; -0.40731832; -0.7954849; … ; -0.6071674; -1.2894586;;; -0.6065134; -0.70599204; … ; -0.90246296; -1.3457153], [-0.19201562; 2.3412762; … ; -0.24236186; 1.9888372;;; 1.7494091; 0.09728269; … ; 1.7381516; 0.4725092;;; -0.08698858; -0.22097667; … ; 0.12678593; 0.6658134;;; -1.1660774; 0.17843252; … ; -0.4528796; -1.2806056;;; 0.1793224; 0.8262942; … ; 1.2158062; 1.9413323;;; -0.8414316; 0.34319237; … ; -0.7721113; 0.37665042;;; 1.1754658; -0.31545982; … ; -0.14475358; 1.0247556;;; -0.97908294; 0.12594631; … ; 0.26525718; -0.89076126])\n",
       " ([-0.19111848; -0.66264206; … ; -0.40864134; -1.0701773;;; -0.21682148; -0.69800305; … ; -0.40909806; -1.0995927;;; 0.08612378; 0.8679841; … ; -0.12704927; 0.873763;;; 0.14391837; 0.67274123; … ; 0.29581475; 0.9967418;;; 0.22488673; 0.6859279; … ; 0.43808508; 1.1069614;;; 0.17841005; 0.7282697; … ; 0.2610401; 1.027518;;; -0.69461775; 0.15388103; … ; -0.8458809; -0.24644454;;; 0.47726947; 0.58395785; … ; 0.8493133; 1.2340026], [-0.43329123; -0.06956691; … ; 0.5439259; 1.8055874;;; -0.16532584; 0.2087579; … ; 0.43336415; -0.12248352;;; -0.8482566; 1.1028068; … ; -0.2846014; -0.3745026;;; 1.0052656; 1.2298441; … ; -1.448303; 0.9180317;;; 0.43518966; 0.21172655; … ; -0.59907657; 1.4140804;;; -0.71374696; -0.96628165; … ; 0.48177764; 1.0157562;;; 0.16260985; -1.032488; … ; 0.44652316; 0.12500332;;; 0.79019874; 1.1504505; … ; 0.048911866; 1.6340096])\n",
       " ([0.080343455; 0.5141054; … ; 0.4151233; 0.93889606;;; 0.26658034; 0.59382784; … ; 0.601812; 1.1217785;;; -0.47157335; -0.9565865; … ; -0.5065982; -1.3591594;;; -0.100873016; -0.6583333; … ; -0.27899534; -0.9973727;;; 0.29103422; 0.68030924; … ; 0.5559757; 1.1695087;;; 0.23610784; 0.76077336; … ; 0.33247834; 1.1000668;;; -0.3290881; -0.9135383; … ; -0.3226206; -1.2067347;;; -0.095248245; -0.72514886; … ; -0.11441225; -0.9116243], [1.2907003; -0.19560356; … ; -0.21732344; 1.0610809;;; -1.1881489; 0.037836142; … ; 0.044388555; 0.19685364;;; -0.2671996; -0.5525317; … ; -0.33838838; -1.9121983;;; -1.5572046; -1.0489204; … ; 1.1419514; 1.0134292;;; -0.23788749; -0.059900116; … ; 0.51736; 1.232356;;; -0.58944803; 0.69780326; … ; 0.75246584; 0.25630277;;; -0.21401384; -1.1257819; … ; -0.9525629; -1.7428116;;; 0.62807727; -0.66322094; … ; -0.23864216; -2.171688])\n",
       " ([-0.4918195; -0.65497464; … ; -0.8320993; -1.2781278;;; -0.1230722; 0.23970842; … ; 0.33089346; 0.63167024;;; 0.2760092; 0.86012363; … ; 0.30921772; 1.1522757;;; 0.48067933; 0.54378694; … ; 0.8888843; 1.2105894;;; 0.14954074; 0.76464105; … ; 0.17447014; 1.004512;;; 0.5669554; 0.31671128; … ; 1.0808198; 1.08702;;; 0.009735309; 0.66726786; … ; -0.5090583; 0.3705055;;; 0.5186024; 1.0305682; … ; 0.5315307; 1.3863671], [-1.1845005; 0.5970352; … ; 2.4780612; -0.0031966292;;; -1.0603273; 0.63826203; … ; -0.95610064; 0.53635865;;; 0.7929349; 1.2338248; … ; -0.054153487; 0.102850325;;; 1.2232633; 0.4346255; … ; -0.24850643; 2.4201324;;; -0.5351178; 0.79340583; … ; 1.0563912; 0.117923364;;; 0.015401761; 0.15027735; … ; 0.46853462; 1.5597998;;; 0.9951579; 0.35610372; … ; 0.17046668; 0.2701007;;; -0.3559076; 0.5721417; … ; -1.5414587; -0.5486478])\n",
       " ([-0.2542058; -0.68943185; … ; -0.48492187; -1.1371578;;; 0.36942983; 0.60040545; … ; 0.73952293; 1.1931483;;; -0.34092262; -0.6630039; … ; -0.647196; -1.2029095;;; 0.10993534; 0.5700625; … ; 0.36084753; 0.96105367;;; -0.21784696; -0.7216123; … ; -0.3553235; -1.08319;;; -0.5413108; 0.14219894; … ; -0.8765325; -0.34703323;;; 0.14956702; 0.723592; … ; 0.23158671; 1.0035965;;; 0.28318018; 0.6923658; … ; 0.5298623; 1.1651058], [1.3272827; 0.5457691; … ; 0.7312778; -0.9396872;;; -0.79048043; 1.1997914; … ; 1.3976731; 0.41337442;;; -0.16692841; -0.28356233; … ; -0.67186695; -0.22507872;;; -0.6833861; -0.021298593; … ; -0.60026896; 1.161227;;; -1.3956238; 0.47941864; … ; 0.6784219; -0.039049596;;; -0.3481523; -0.06413461; … ; -1.6848048; -1.1409992;;; 0.24042167; 0.21482058; … ; 0.5843378; -1.403219;;; 0.5179703; 0.37675476; … ; -0.07157317; 0.4534033])\n",
       " ⋮\n",
       " ([-0.36158803; -0.6553125; … ; -0.68229103; -1.2136804;;; -0.1946395; -0.74736035; … ; -0.28631955; -1.062711;;; -0.780393; -0.7383079; … ; -0.9921494; -1.3787966;;; 0.11759059; 0.66384685; … ; 0.25364172; 0.97371477;;; 0.34637693; 0.7036378; … ; 0.61612934; 1.2205861;;; 0.2002315; 0.6828145; … ; 0.39677456; 1.0796288;;; -0.025098503; 0.88480026; … ; -0.34781766; 0.74176306;;; -0.32410553; -0.66207457; … ; -0.62451684; -1.1906382], [-0.5398753; -1.4126112; … ; -0.44719142; -0.797975;;; 0.77315366; 0.52903354; … ; 0.54744077; -0.24229415;;; 0.07361873; 0.10448889; … ; -1.6503097; -0.9883159;;; -0.08067503; -0.7349008; … ; 1.1216863; -1.2757853;;; 0.74231654; 0.03243579; … ; 0.23809889; -0.38202912;;; -0.11609371; 1.3781; … ; -0.41463777; 1.1040286;;; 0.14373621; 0.93748236; … ; 0.7158303; 1.4202297;;; -0.36058116; -0.23471814; … ; 0.54489344; -1.8696601])\n",
       " ([-0.009797647; -0.4002919; … ; -0.42334664; -0.8469861;;; -0.42909014; -0.7131627; … ; -0.69852144; -1.2692754;;; -0.20176701; -0.7806006; … ; -0.22856775; -1.0352368;;; -0.19681613; -0.6955252; … ; -0.37071106; -1.0739018;;; 0.25812435; 0.6849149; … ; 0.49651712; 1.1401412;;; -0.11703484; -0.6473693; … ; -0.28053305; -0.9736787;;; 0.17232886; -0.80175674; … ; 0.6143175; -0.4622838;;; 0.22792177; 0.7939458; … ; 0.2952609; 1.1058818], [-0.39294106; 1.765273; … ; -0.4692353; -1.2398138;;; -1.3835368; -1.6256773; … ; -0.8872889; -0.028749965;;; -0.5690118; 0.69458884; … ; -1.7531259; 0.9299648;;; 0.2626719; -0.14348905; … ; -0.9716471; -1.133596;;; -0.21620408; 0.053745646; … ; 1.8771684; -0.05846021;;; 0.45942694; -1.8036195; … ; -0.52311057; -0.052829534;;; -0.12520018; 0.004408733; … ; 0.37448835; -1.3683542;;; -0.89446265; 0.3309739; … ; 0.8173282; 1.7183478])\n",
       " ([0.58512783; 0.5509552; … ; 0.9856685; 1.2581294;;; -0.20568286; -0.7011258; … ; -0.4069776; -1.087723;;; -0.7526542; -0.107570164; … ; -1.1201514; -0.76392984;;; 0.1816647; 0.6741464; … ; 0.37335393; 1.058227;;; -0.5646802; -0.09418603; … ; -1.057239; -0.7809663;;; 0.15303497; 0.7988489; … ; 0.14105032; 0.9893762;;; -0.6774624; -0.5800173; … ; -1.0273297; -1.2696136;;; -0.18283077; -0.6452197; … ; -0.40663856; -1.053111], [-0.20251188; 0.90953386; … ; -0.76050514; 1.3886156;;; -0.23986453; 0.43535402; … ; -0.12712182; 0.60203993;;; -0.78416693; 0.47422025; … ; -1.7465774; -0.50412136;;; 0.20132168; 0.19306786; … ; -2.1081572; 0.9011482;;; -1.7583117; 0.6466903; … ; 0.08389974; -0.6167722;;; -0.35615036; 0.59564894; … ; 0.60525817; 0.97745734;;; 0.5298723; 0.36648056; … ; 0.7521535; -0.38014;;; 0.14344275; 1.2321426; … ; -1.2614702; -0.48887157])\n",
       " ([-0.7414371; -0.32211655; … ; -1.1188418; -1.0059558;;; 0.42848748; -0.39811367; … ; 1.1107728; 0.32558137;;; 0.3187302; 0.757583; … ; 0.5063601; 1.204635;;; -0.39421523; -0.9202117; … ; -0.4189861; -1.2812408;;; -0.10585143; -0.6502493; … ; -0.24885945; -0.9589529;;; 0.3136489; -0.46212655; … ; 1.1368378; 0.34297138;;; 0.8458435; 0.91304123; … ; 0.8694998; 1.452274;;; 0.22618337; 0.6974426; … ; 0.41749862; 1.1010131], [0.40997827; 2.0594444; … ; -2.2125297; -0.15908885;;; 1.1910158; -0.958004; … ; -0.5293675; 0.2998835;;; -0.96931165; 0.22194527; … ; -1.508808; 0.4475308;;; 0.2721665; -0.13072518; … ; -1.8146319; 0.057400882;;; -0.22057022; 0.05412371; … ; 0.32450888; -0.3788392;;; 1.400323; 0.7922998; … ; 0.864643; -0.78990674;;; -1.1609; 0.4927524; … ; 1.0233707; -0.22279206;;; 0.06893709; 0.47737005; … ; 0.69112664; 0.29910895])\n",
       " ([-0.20072004; -0.68522984; … ; -0.39428937; -1.0800889;;; 0.36512956; 0.6238305; … ; 0.7130803; 1.2018764;;; 0.2877783; 0.58976907; … ; 0.6362075; 1.1334616;;; -0.093345776; -0.2729265; … ; -0.5778714; -0.849744;;; 0.09213099; 0.70456666; … ; 0.19424368; 0.9625291;;; 0.3889096; 0.5987678; … ; 0.74996793; 1.1915616;;; 0.23658499; 0.6762419; … ; 0.47116134; 1.1183703;;; 0.32444927; 0.68607783; … ; 0.6005109; 1.1980959], [-0.119795136; -0.19366919; … ; -1.1218528; 0.0064777196;;; 0.5733302; 0.99662626; … ; 0.28572762; 1.027833;;; 0.25141373; 0.52471405; … ; 1.1755286; 0.15851024;;; 0.31108963; -0.44345585; … ; 0.388364; -0.46697044;;; 2.135312; -0.8462417; … ; -1.1246321; 0.04303042;;; 0.6630344; 1.3040515; … ; 1.2681261; -0.6856403;;; 0.38947603; -0.2865257; … ; -0.23521568; 0.3175943;;; 0.2430818; 0.17330201; … ; 0.7879481; 0.6825224])\n",
       " ([0.15083218; 0.6770088; … ; 0.30606207; 1.0183876;;; -0.30296975; -0.62736976; … ; -0.6295425; -1.1639868;;; 0.29157338; 0.6769683; … ; 0.55965614; 1.168693;;; 0.4083771; 0.7553654; … ; 0.64564437; 1.2777854;;; -0.25770706; -0.7653898; … ; -0.38571626; -1.1394008;;; 0.16791788; 0.67369705; … ; 0.3465069; 1.0411975;;; 0.16465333; 0.65715873; … ; 0.40476164; 1.0413305;;; 0.46226862; -0.4141952; … ; 0.94775516; 0.14556028], [0.99237347; -0.8013265; … ; 0.8856825; -0.32743934;;; 0.22746271; -1.556304; … ; 0.9194575; 0.120722264;;; 1.0042111; 1.6444002; … ; 1.0966353; 0.22701204;;; 0.71202415; -1.7542073; … ; 0.30127504; -0.5280615;;; -0.9356112; -0.5941982; … ; 0.38067257; 0.9586781;;; -0.5410368; -0.7414483; … ; -0.9056293; -0.72269166;;; -0.55106944; -0.32606074; … ; 0.4308714; 0.20800823;;; 0.9641177; 0.40601894; … ; 0.8378632; -0.33103552])\n",
       " ([0.10124172; 0.65786314; … ; 0.22555056; 0.95016354;;; -0.11435947; -0.6702613; … ; -0.23487464; -0.96643686;;; -0.21418318; -0.68213093; … ; -0.42371193; -1.0952914;;; -0.23126028; -0.6799896; … ; -0.45725974; -1.1133757;;; -0.21237421; -0.7285847; … ; -0.35073808; -1.0880795;;; 0.61316156; 0.22910169; … ; 1.1322994; 1.0284392;;; 0.026805094; 0.7614076; … ; -0.35810444; 0.591458;;; 0.03516731; 0.70820457; … ; 0.08126628; 0.8810003], [1.348957; -0.5064199; … ; 1.1419063; -0.29536068;;; 0.67232823; 0.4451201; … ; 0.06488712; 1.24732;;; 0.5966124; -0.5053511; … ; -0.8028846; 0.3724115;;; 0.8934703; 0.32885978; … ; -0.20174915; -0.9998691;;; -0.94245654; -0.48912144; … ; -0.5205142; 0.27966443;;; -0.006811752; 0.94887275; … ; 0.26564565; -0.95215505;;; 0.30716926; 1.0477251; … ; 2.587153; 2.6321044;;; 1.5296382; -0.24985138; … ; -0.29780188; 0.15377179])\n",
       " ([-0.21833032; -0.69763184; … ; -0.41099074; -1.1003566;;; -0.48084193; 0.1436034; … ; -0.86369956; -0.34192064;;; 0.29646546; 0.70323586; … ; 0.53789276; 1.1784742;;; -0.18107477; -0.71047455; … ; -0.31932643; -1.0541043;;; -0.16887838; -0.94340646; … ; 0.056886442; -0.9815062;;; 0.3127654; 0.63411117; … ; 0.63493973; 1.1727655;;; -0.34412822; -0.6500637; … ; -0.6639267; -1.2003793;;; -0.25682855; -0.6755691; … ; -0.50659525; -1.1381428], [-0.8943945; -0.11215023; … ; -0.8338214; -1.1306494;;; -0.42668363; 1.1362959; … ; -1.6096519; 0.20555176;;; 0.6670454; -0.008400352; … ; 0.77720284; 1.134467;;; 0.83586955; 0.08024398; … ; -1.7798985; -1.3443614;;; 1.3973119; -0.36657515; … ; -0.49060887; -1.8984739;;; 1.3775852; 0.6639001; … ; 0.58244616; 0.6280617;;; -0.37074226; 0.15812941; … ; -0.22042456; -1.2294445;;; 0.13794687; -0.34393862; … ; -0.0975248; 0.48482484])\n",
       " ([-0.6013288; -0.8840473; … ; -0.6599656; -1.3460941;;; 0.1980768; 0.74694467; … ; 0.28557867; 1.0604688;;; 0.3279259; 0.6742528; … ; 0.6109961; 1.1852713;;; 0.7264624; 0.1660548; … ; 0.44240728; 0.23989493;;; 0.10173196; 0.6748294; … ; 0.21018703; 0.95360374;;; -0.17615521; -0.6850988; … ; -0.34628743; -1.0503843;;; -0.48252937; -1.1722506; … ; 0.117524095; -1.0546273;;; -0.7561688; -0.805899; … ; -0.9089515; -1.4019426], [-0.8382229; 1.2903827; … ; -0.18987976; -1.2116771;;; 0.36360094; 1.4156957; … ; 0.6102447; -0.8002005;;; 1.4725221; 1.3335407; … ; -1.2298058; 0.5377467;;; 1.6003863; -0.6655884; … ; 0.72921133; 0.7560614;;; 0.90945935; 0.07067673; … ; 0.37927267; 0.6521006;;; 0.12511668; 1.5097752; … ; -0.76369137; -0.9528296;;; -0.23239379; 0.11708815; … ; -2.0259173; 0.733859;;; -0.84424996; -0.17056969; … ; -1.3419458; -0.34872058])\n",
       " ([-0.33575046; -0.6601101; … ; -0.6428077; -1.1982837;;; -0.68706435; 0.07149709; … ; -0.9457811; -0.41335583;;; -0.8710188; -0.872843; … ; -0.88487744; -1.4078667;;; 0.3296189; 0.678727; … ; 0.6158077; 1.1999722;;; -0.4836337; -0.611343; … ; -0.85546666; -1.2525142;;; 0.119432665; 0.7603393; … ; 0.094242446; 0.9450709;;; 0.30445912; 0.78040135; … ; 0.46127543; 1.1974608;;; -0.3026322; -0.83027816; … ; -0.116377816; -0.9556081], [0.3035456; 1.5157264; … ; -1.3566158; 0.73588955;;; 0.033494044; -1.4732568; … ; 1.4872589; -0.12319873;;; -0.263798; 0.5445903; … ; -0.31206805; -0.89834714;;; -1.0901303; 0.7486078; … ; 1.5664043; 1.9102913;;; 0.5522134; -1.2555003; … ; -1.5482823; -1.7024629;;; -0.9667659; -0.27314094; … ; -1.0905445; -0.38954583;;; -0.57514423; 0.34895584; … ; 0.09047998; 0.018033937;;; 0.48792917; -0.8184599; … ; -2.6204364; -0.507393])\n",
       " ([-0.12562644; -0.6799617; … ; -0.25211248; -0.98634136;;; 0.8389159; 0.8526515; … ; 0.93676424; 1.4388558;;; -0.2983707; -0.6499027; … ; -0.6001615; -1.1678106;;; -0.6468776; -0.4805685; … ; -1.0365645; -1.1657689;;; -0.29776403; -0.666437; … ; -0.57894784; -1.1574945;;; -0.7555408; -0.54873097; … ; -1.0871367; -1.2460525;;; -0.621212; -0.8554015; … ; -0.8014738; -1.4135213;;; -0.39353478; -0.9344366; … ; -0.41489407; -1.2902954], [0.17687608; 0.31542516; … ; -0.42127433; -0.054823138;;; 0.8180628; 0.08895713; … ; 1.4818734; 0.19806021;;; 0.46625164; -0.82410765; … ; -1.0154008; 0.07556497;;; 1.3140838; -0.42138794; … ; 0.25385928; -1.0408645;;; 0.94191813; 1.7414154; … ; 0.9611492; -0.28651837;;; 0.6793162; -0.5937517; … ; 0.20379747; 0.450868;;; -0.21713585; 0.087707445; … ; -1.8085213; -1.4944528;;; -0.6582659; -1.618108; … ; -0.35239458; -1.4093611])\n",
       " ([0.24301302; 0.67524683; … ; 0.48321286; 1.1245012;;; 0.32045755; 0.7044111; … ; 0.57775295; 1.2004522;;; 0.42145926; 0.6516381; … ; 0.75689304; 1.2443433;;; 0.4523585; 0.6967459; … ; 0.7552944; 1.2696134;;; 0.5583872; 1.0289232; … ; 0.4903931; 1.3919895], [2.2590518; 0.6392936; … ; -0.89397585; 0.7590391;;; 0.29917884; 0.5138969; … ; 0.21536234; 1.0997689;;; 0.696049; 0.073833555; … ; -0.18674599; 2.433206;;; -0.1660604; 0.9008194; … ; 1.4217143; 0.60982317;;; 1.0579953; 0.17383674; … ; -1.4883541; -0.31671557])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = collect(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3af09da6-0f0b-4068-bcaa-9e34f01be7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3727925903620586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negr2(train_data[1][1],train_data[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e91c31-e7f2-4366-afa3-7c290e942283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "    layer_1 = Conv((9,), 1 => 18, σ, pad=4),  \u001b[90m# 180 parameters\u001b[39m\n",
       "    layer_2 = Chain(\n",
       "        layer_1 = Conv((9,), 18 => 18, pad=4),  \u001b[90m# 2_934 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(18, σ, affine=true, track_stats=true),  \u001b[90m# 36 parameters\u001b[39m\u001b[90m, plus 37\u001b[39m\n",
       "    ),\n",
       "    layer_3 = Chain(\n",
       "        layer_1 = Conv((9,), 18 => 18, pad=4),  \u001b[90m# 2_934 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(18, σ, affine=true, track_stats=true),  \u001b[90m# 36 parameters\u001b[39m\u001b[90m, plus 37\u001b[39m\n",
       "    ),\n",
       "    layer_4 = Chain(\n",
       "        layer_1 = Conv((9,), 18 => 18, pad=4),  \u001b[90m# 2_934 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(18, σ, affine=true, track_stats=true),  \u001b[90m# 36 parameters\u001b[39m\u001b[90m, plus 37\u001b[39m\n",
       "    ),\n",
       "    layer_5 = Chain(\n",
       "        layer_1 = Conv((9,), 18 => 18, pad=4),  \u001b[90m# 2_934 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(18, σ, affine=true, track_stats=true),  \u001b[90m# 36 parameters\u001b[39m\u001b[90m, plus 37\u001b[39m\n",
       "    ),\n",
       "    layer_6 = Chain(\n",
       "        layer_1 = Conv((9,), 18 => 18, pad=4),  \u001b[90m# 2_934 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(18, σ, affine=true, track_stats=true),  \u001b[90m# 36 parameters\u001b[39m\u001b[90m, plus 37\u001b[39m\n",
       "    ),\n",
       "    layer_7 = Chain(\n",
       "        layer_1 = Conv((9,), 18 => 18, pad=4),  \u001b[90m# 2_934 parameters\u001b[39m\n",
       "        layer_2 = BatchNorm(18, σ, affine=true, track_stats=true),  \u001b[90m# 36 parameters\u001b[39m\u001b[90m, plus 37\u001b[39m\n",
       "    ),\n",
       "    layer_8 = Dropout(0.2),\n",
       "    layer_9 = Conv((1,), 18 => 1),      \u001b[90m# 19 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: \u001b[39m18_019 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m224 states."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "        Conv((9,), 1=>18, sigmoid, pad=SamePad()),\n",
    "        [Chain(Conv((9,), 18=>18, pad=SamePad()), BatchNorm(18, sigmoid)) for _ in 1:6]...,\n",
    "        Dropout(0.2),\n",
    "        Conv((1,), 18=>1, pad=SamePad())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15f706d2-940b-4b2e-a23e-edbdfa667326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((layer_1 = (weight = Float32[-0.105983034; -0.17700903; … ; 0.17689286; 0.5762858;;; -0.030952485; -0.2516598; … ; 0.1556233; -0.19918957;;; 0.013296677; 0.47829136; … ; 0.44485468; -0.038110614;;; … ;;; -0.5703142; -0.3655195; … ; -0.14900751; -0.2651057;;; -0.19353914; 0.2612538; … ; 0.021623876; -0.07029934;;; -0.10583988; 0.5010744; … ; 0.5380823; 0.3065268], bias = Float32[-0.17057855, 0.24381645, 0.10418979, -0.10502354, 0.24592765, -0.30111828, 0.16615231, 0.070468344, 0.08851385, 0.1231846, -0.29632887, -0.21839802, 0.30791894, 0.1067009, -0.2811664, 0.09673294, 0.26899657, -0.15632947]), layer_2 = (layer_1 = (weight = Float32[0.026080474 -0.055631038 … -0.123914324 -0.031135645; 0.13578852 0.05806497 … 0.09317774 -0.09072603; … ; 0.04616294 0.07360015 … 0.0073334333 -0.07687018; 0.023676682 0.12932956 … -0.052710045 0.07514027;;; 0.009319014 -0.018593287 … 0.13592397 0.04805164; -0.014001005 0.025715278 … -0.04884728 0.10543372; … ; 0.056692075 0.065480456 … 0.119357854 0.07568287; 0.11644835 0.09077758 … -0.040859926 0.029067915;;; 0.076345034 -0.06482284 … -0.039992258 0.05955736; 0.036294084 0.08141523 … -0.014988101 -0.121716455; … ; -0.06488136 -0.0064738444 … 0.092632025 -0.04383893; -0.039183997 -0.06203214 … 0.13125564 0.09828531;;; … ;;; -0.10434497 0.021895634 … 0.0687233 -0.056415547; 0.06751746 -0.11282168 … -0.0096290875 -0.07217408; … ; -0.0826175 0.1051543 … -0.016946947 0.050934445; -0.12677191 0.03722424 … -0.12382636 -0.018110413;;; 0.035875097 -0.11869534 … -0.13326961 0.047095302; 0.08399438 -0.025781563 … -0.092390895 0.039896935; … ; -0.107762106 -0.109024756 … -0.1358452 -0.095252775; -0.052451752 -0.1183998 … -0.095659114 -0.06892884;;; 0.027565727 0.06702868 … -0.057393655 0.13244843; 0.06457745 0.0314682 … -0.019499466 0.073460706; … ; 0.13183819 -0.03826273 … 0.10318303 0.12741905; -0.07260229 0.10664332 … 0.040922705 0.08305287], bias = Float32[-0.06008816, -0.0014521931, 0.068433724, 0.004380238, -0.05100486, -0.0656966, -0.051115025, 0.061690547, 0.056497212, -0.013493248, 0.06392461, -0.011827324, -0.06168407, -0.058897167, -0.04722577, -0.029446917, -0.05745024, -0.014944524]), layer_2 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])), layer_3 = (layer_1 = (weight = Float32[0.05147562 -0.024898386 … 0.0749467 -0.12737066; 0.07735757 -0.07351232 … -0.0068464065 0.0881959; … ; -0.11344745 -0.026647413 … -0.016025519 0.029175371; -0.07336078 0.11754608 … 0.028589193 -0.00894045;;; -0.06417031 -0.059137654 … 0.08263437 0.07321085; -0.08150332 0.061092477 … -0.06863988 0.068648316; … ; -0.09957389 0.03210827 … 0.06590123 0.1067682; -0.0053539523 0.030014195 … -0.100223266 -0.07166149;;; 0.07318723 0.07103365 … -0.07942984 -0.059723604; -0.12666109 0.10358278 … 0.0059773116 -0.10190108; … ; 0.04924823 0.03949676 … -0.071008116 0.040848017; -0.09722561 -0.013315287 … -0.018736238 0.06955683;;; … ;;; -0.03575135 -0.039398875 … 0.040127616 0.061590146; -0.10415448 -0.014429404 … -0.099755414 -0.07317763; … ; 0.017422294 -0.08510396 … -0.13256335 0.015861252; 0.04499782 -0.02889677 … 0.09879644 0.082524024;;; -0.12192462 -0.09549666 … 0.04035382 0.076493174; 0.032837074 -0.09810924 … 0.033211518 0.09994934; … ; 0.046041466 0.1171694 … 0.06312578 -0.030084891; 0.081676014 -0.067458704 … -0.051094756 0.0646818;;; 0.031247675 -0.060813162 … -0.05900168 0.0736641; -0.12679079 0.05035982 … 0.023984421 -0.119624324; … ; -0.039271887 0.029842304 … 0.010561255 -0.052901693; -0.016703483 -0.009040801 … -0.11184056 0.006431991], bias = Float32[-0.008935582, -0.067064196, -0.016197389, 0.06662731, 0.041939244, 0.072540686, 0.03910892, -0.03299802, -0.027030198, -0.02213086, -0.027248837, -0.007830117, 0.021715555, 0.066693395, 0.033664823, 0.011266434, -0.06314778, -0.04690431]), layer_2 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])), layer_4 = (layer_1 = (weight = Float32[-0.10420802 -0.050768133 … -0.03682582 -0.06568771; -0.02241001 -0.13464029 … -0.033756133 0.041874144; … ; 0.038972393 0.08015472 … 0.075016424 -0.0978111; 0.0069146054 -0.08157453 … -0.097098485 -0.013008912;;; -0.120002955 -0.0066188397 … 0.08402034 -0.12287389; 0.13408938 0.020187909 … -0.099253885 0.08076117; … ; 0.0014101745 -0.11323617 … 0.102818094 0.09524959; 0.08725523 0.0055430396 … -0.076780505 -0.07692495;;; 0.11248047 0.10760332 … 0.0626179 0.006116272; 0.07445767 -0.11678701 … 0.075858235 -0.016485065; … ; -0.13572097 0.12317296 … 0.09334279 -0.057249278; -0.10906599 0.033478927 … 0.11529118 0.05845385;;; … ;;; -0.08780783 0.08150539 … 0.08532938 0.100407615; 0.032441184 0.10550464 … -0.030512415 0.029212099; … ; -0.021382619 0.04398616 … -0.07408212 -0.08340399; -0.120662846 0.08930116 … 0.109516844 0.059784666;;; -0.1323634 0.10205798 … -0.09689736 -0.11416003; -0.11186259 -0.124647826 … -0.03165622 -0.02855876; … ; 0.02850542 0.027847184 … -0.106216244 -0.07048348; 0.1101404 0.112528585 … 0.06884289 -0.07880071;;; -0.06856393 -0.045468625 … -0.03419735 0.019035345; 0.031032244 -0.07322671 … 0.12689906 0.0763086; … ; 0.108740866 0.06060302 … -0.079983704 -0.046181466; -0.07939818 -0.01876508 … -0.13464274 0.04841463], bias = Float32[0.021349365, 0.013386663, -0.03074635, -0.023716986, -0.06545287, 0.061558057, -0.031665977, -0.050858602, 0.015470366, -0.071138166, 0.023542762, 0.0016496464, 0.04523923, -0.07204646, -0.026820475, 0.037682094, 0.040651683, 0.016856417]), layer_2 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])), layer_5 = (layer_1 = (weight = Float32[0.07637433 -0.016955903 … 0.09745875 -0.01717792; 0.045926515 0.048751082 … 0.12161208 -0.091313444; … ; -0.064605065 0.100164995 … -0.06956164 0.026215866; 0.10591568 0.055902664 … -0.12598851 -0.054387398;;; 0.059107352 -0.050374225 … 0.10179566 -0.10008768; -0.072121 -0.03952116 … 0.015093449 0.063410744; … ; 0.11128553 0.12710616 … 0.007304493 0.029689781; 0.024050282 -0.04343668 … -0.07273301 0.10165209;;; -0.031062678 0.12779985 … -0.110633165 0.12044352; 0.12438454 0.12987094 … 0.027925862 0.07822566; … ; 0.10535529 -0.050636895 … 0.12637304 -0.016117955; -0.08534064 -0.031180743 … -0.13289104 0.05759978;;; … ;;; 0.048417225 -0.009048295 … 0.020437019 -0.043595076; 0.0787326 -0.034029093 … 0.12211416 -0.066931315; … ; -0.074958384 0.06962377 … 0.072617374 -0.13517053; -0.113219164 0.106197394 … -0.026953561 0.1321807;;; -0.08300654 0.019235367 … 0.037084144 0.048016924; -0.06989355 0.0033269725 … 0.12887754 -0.055750042; … ; -0.0796187 -0.103341945 … -0.08033151 -0.06237725; -0.1192213 -0.027284984 … -0.08666886 0.10352886;;; -0.079016946 -0.007909715 … 0.06377909 0.10151648; 0.10564911 -0.024252381 … -0.024726884 0.027208868; … ; 0.05112467 -0.1351804 … 0.045602232 -0.0984513; -0.023486039 0.041171554 … -0.07027724 -0.09058253], bias = Float32[-0.02103819, -0.044197377, 0.06398632, 0.037587084, 0.051188696, 0.04169863, 0.042717367, -0.032242097, -0.0029235864, -0.032016955, -0.032567862, -0.053550024, -0.019222485, 0.036495913, -0.007938087, 0.043938596, -0.009441587, 0.03238901]), layer_2 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])), layer_6 = (layer_1 = (weight = Float32[0.081134155 0.0595741 … 0.13523225 -0.07111398; -0.06697333 0.10980651 … -0.04921105 0.06764931; … ; -0.12019428 0.08776257 … -0.008089362 0.09758811; 0.053646624 -0.10421801 … 0.08812299 -0.06373886;;; -0.09005683 -0.027564494 … 0.09759515 0.026529606; -0.02644181 -0.010729286 … -0.00808576 0.11441861; … ; -0.08348533 -0.0659485 … -0.09264666 -0.08784157; 0.13229954 -0.032363255 … 0.10233852 0.015183353;;; 0.102085 0.06588342 … 0.025645847 0.013267821; 0.05629044 0.11097205 … 0.1287392 0.11081083; … ; 0.080090865 0.035881843 … -0.06337395 0.056359485; 0.082961865 0.036161873 … 0.04874774 -0.12436848;;; … ;;; -0.056215625 -0.06134389 … 0.075719826 -0.09081363; 0.10010835 0.12727211 … 0.075219855 -0.11837715; … ; 0.048159745 -0.12111496 … 0.068052895 -0.084736325; 0.0013471021 -0.032071576 … 0.003687757 0.12876818;;; 0.028783148 -0.08301329 … -0.0999713 0.07567944; -0.03406689 0.036473926 … -0.013293939 0.13322973; … ; -0.106038965 0.0893798 … 0.027134797 -0.03796849; 0.027335208 -0.0967244 … 0.003259455 -0.12138653;;; 0.113504715 0.02997656 … 0.07107022 -0.06542563; -0.12713055 0.11377082 … 0.096230626 0.13444401; … ; -0.0743979 0.13463277 … -0.1299995 0.068408325; 0.0067202616 -0.08881848 … -0.09802017 -0.11454719], bias = Float32[-0.07202387, -0.011691686, -0.06444671, 0.0151531035, 0.047275335, 0.07477175, 0.03610848, -0.02614843, -0.054420274, 0.034327164, 0.029858157, 0.04852981, 0.036400698, 0.04087896, 0.013912712, 0.020957923, 0.041419543, -0.00076791557]), layer_2 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])), layer_7 = (layer_1 = (weight = Float32[-0.026814569 -0.05951531 … 0.0009756108 0.013825382; 0.052234113 -0.060098667 … 0.13037315 0.061296295; … ; -0.047199644 0.05816503 … 0.024098106 -0.09063155; 0.11765572 0.057654902 … -0.09026999 -0.04871669;;; 0.039176665 0.0001123234 … -0.03350086 -0.03921274; -0.127376 0.08719573 … -0.0003020273 -0.0047501246; … ; -0.05474027 0.10912955 … 0.01733544 -0.03476104; -0.03921923 0.10061 … 0.11921623 0.114365205;;; -0.09664696 0.10553815 … -0.04433916 0.011514544; -0.04617559 0.06688736 … -0.05694352 -0.118350126; … ; -0.059177108 -0.024970673 … -0.07010317 0.0013036263; 0.062420666 0.02038222 … 0.019584147 -0.07026572;;; … ;;; 0.098744534 -0.021615118 … -0.008724108 0.008927374; 0.0887606 0.09703496 … -0.08670199 0.08580531; … ; 0.064664476 -0.020161467 … -0.116309516 0.0015295709; 0.10061471 -0.022844672 … 0.10670376 -0.028605806;;; 0.019838188 -0.023493955 … 0.03413499 0.005983022; -0.12168706 0.041102093 … 0.06842147 0.032794863; … ; -0.012111461 -0.08114957 … -0.01787399 -0.113240354; 0.0546661 -0.08448348 … -0.024341896 -0.057027422;;; 0.06432747 -0.10210586 … -0.016985362 -0.08221729; 0.09220908 -0.12831797 … 0.0109612 -0.05179043; … ; -0.036137346 -0.041254807 … 0.0824312 -0.027070524; 0.09850931 -0.11411779 … 0.06706372 -0.09751803], bias = Float32[0.022695534, -0.01585645, -0.024966164, 0.014578145, 0.019064236, 0.050947446, 0.033281155, -0.023103721, 0.009336651, -0.03668295, 0.03475191, 0.072667964, -0.045444287, 0.034511153, -0.013042183, 0.035805788, 0.041978072, 0.065972894]), layer_2 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])), layer_8 = NamedTuple(), layer_9 = (weight = Float32[0.08398346 0.34029594 … 0.25550735 0.086581305;;;], bias = Float32[-0.15832116])), (layer_1 = NamedTuple(), layer_2 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_3 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_4 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_5 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_6 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_7 = (layer_1 = NamedTuple(), layer_2 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}())), layer_8 = (rng = MersenneTwister(12345, (0, 9738, 0, 176)), training = Val{true}()), layer_9 = NamedTuple()))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps, st = Lux.setup(rng, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca05960d-05ea-41d2-b2ba-33cc2c5cfb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(1.0e-5, (0.9, 0.999), 1.0e-8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Adam(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9f4930-9645-42ba-b14f-3c6058633127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState\n",
       "    model: Chain{@NamedTuple{layer_1::Conv{typeof(σ), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_3::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_4::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_5::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_6::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_7::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_8::Dropout{Float64, Colon}, layer_9::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}}, Nothing}((layer_1 = Conv((9,), 1 => 18, σ, pad=4), layer_2 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_3 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_4 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_5 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_6 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_7 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_8 = Dropout(0.2), layer_9 = Conv((1,), 18 => 1)), nothing)\n",
       "    # of parameters: 18019\n",
       "    # of states: 224\n",
       "    optimizer: Adam(1.0e-5, (0.9, 0.999), 1.0e-8)\n",
       "    step: 0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstate = Training.TrainState(model, ps, st, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb230ce3-1539-40ba-bef4-08557860297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`training` is set to `Val{true}()` but is not being used within an autodiff call (gradient, jacobian, etc...). This will be slow. If you are using a `Lux.jl` model, set it to inference (test) mode using `LuxCore.testmode`. Reliance on this behavior is discouraged, and is not guaranteed by Semantic Versioning, and might be removed without a deprecation cycle. It is recommended to fix this issue in your code.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ LuxLib.Utils ~/.julia/packages/LuxLib/ru5RQ/src/utils.jl:314\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 \t Current Loss: 1.8676 \t Best Loss: 1.8661 \t Test Loss: 1.8755 \t Fidelity: -0.0056623\n",
      "Epoch:   2 \t Current Loss: 1.8468 \t Best Loss: 1.7513 \t Test Loss: 1.8053 \t Fidelity: 0.073875\n",
      "Epoch:   3 \t Current Loss: 1.7458 \t Best Loss: 1.6456 \t Test Loss: 1.7658 \t Fidelity: 0.10861\n",
      "Epoch:   4 \t Current Loss: 1.7824 \t Best Loss: 1.6413 \t Test Loss: 1.7398 \t Fidelity: 0.12677\n",
      "Epoch:   5 \t Current Loss: 1.6719 \t Best Loss: 1.5851 \t Test Loss: 1.7178 \t Fidelity: 0.13373\n",
      "Epoch:   6 \t Current Loss: 1.7342 \t Best Loss: 1.5247 \t Test Loss: 1.6959 \t Fidelity: 0.1325\n",
      "Epoch:   7 \t Current Loss: 1.7416 \t Best Loss: 1.5247 \t Test Loss: 1.6761 \t Fidelity: 0.13866\n",
      "Epoch:   8 \t Current Loss: 1.6162 \t Best Loss: 1.5247 \t Test Loss: 1.6568 \t Fidelity: 0.14417\n",
      "Epoch:   9 \t Current Loss: 1.5404 \t Best Loss: 1.5061 \t Test Loss: 1.6411 \t Fidelity: 0.13924\n",
      "Epoch:  10 \t Current Loss: 1.5727 \t Best Loss: 1.5019 \t Test Loss: 1.6203 \t Fidelity: 0.14016\n",
      "Epoch:  11 \t Current Loss: 1.6467 \t Best Loss: 1.4679 \t Test Loss: 1.6039 \t Fidelity: 0.13819\n",
      "Epoch:  12 \t Current Loss: 1.6123 \t Best Loss: 1.4164 \t Test Loss: 1.5847 \t Fidelity: 0.14245\n",
      "Epoch:  13 \t Current Loss: 1.6242 \t Best Loss: 1.4164 \t Test Loss: 1.5675 \t Fidelity: 0.14619\n",
      "Epoch:  14 \t Current Loss: 1.4391 \t Best Loss: 1.3688 \t Test Loss: 1.5523 \t Fidelity: 0.13911\n",
      "Epoch:  15 \t Current Loss: 1.4979 \t Best Loss: 1.3688 \t Test Loss: 1.5355 \t Fidelity: 0.14282\n",
      "Epoch:  16 \t Current Loss: 1.5453 \t Best Loss: 1.3206 \t Test Loss: 1.5154 \t Fidelity: 0.14661\n",
      "Epoch:  17 \t Current Loss: 1.5445 \t Best Loss: 1.3206 \t Test Loss: 1.4997 \t Fidelity: 0.14329\n",
      "Epoch:  18 \t Current Loss: 1.4675 \t Best Loss: 1.3206 \t Test Loss: 1.4839 \t Fidelity: 0.14701\n",
      "Epoch:  19 \t Current Loss: 1.4267 \t Best Loss: 1.3206 \t Test Loss: 1.4687 \t Fidelity: 0.14579\n",
      "Epoch:  20 \t Current Loss: 1.5243 \t Best Loss: 1.3206 \t Test Loss: 1.4542 \t Fidelity: 0.14695\n",
      "Epoch:  21 \t Current Loss: 1.4205 \t Best Loss: 1.3105 \t Test Loss: 1.4401 \t Fidelity: 0.14692\n",
      "Epoch:  22 \t Current Loss: 1.4105 \t Best Loss: 1.2905 \t Test Loss: 1.4252 \t Fidelity: 0.14091\n",
      "Epoch:  23 \t Current Loss: 1.4157 \t Best Loss: 1.26 \t Test Loss: 1.4107 \t Fidelity: 0.14468\n",
      "Epoch:  24 \t Current Loss: 1.2987 \t Best Loss: 1.236 \t Test Loss: 1.3993 \t Fidelity: 0.14617\n",
      "Epoch:  25 \t Current Loss: 1.2574 \t Best Loss: 1.236 \t Test Loss: 1.3851 \t Fidelity: 0.14503\n",
      "Epoch:  26 \t Current Loss: 1.3468 \t Best Loss: 1.1742 \t Test Loss: 1.3708 \t Fidelity: 0.14287\n",
      "Epoch:  27 \t Current Loss: 1.3045 \t Best Loss: 1.1742 \t Test Loss: 1.358 \t Fidelity: 0.14518\n",
      "Epoch:  28 \t Current Loss: 1.3375 \t Best Loss: 1.1742 \t Test Loss: 1.3449 \t Fidelity: 0.14348\n",
      "Epoch:  29 \t Current Loss: 1.1661 \t Best Loss: 1.1661 \t Test Loss: 1.3346 \t Fidelity: 0.14252\n",
      "Epoch:  30 \t Current Loss: 1.291 \t Best Loss: 1.1163 \t Test Loss: 1.3182 \t Fidelity: 0.14752\n",
      "Epoch:  31 \t Current Loss: 1.3339 \t Best Loss: 1.1163 \t Test Loss: 1.3071 \t Fidelity: 0.14582\n",
      "Epoch:  32 \t Current Loss: 1.2201 \t Best Loss: 1.1163 \t Test Loss: 1.2941 \t Fidelity: 0.14097\n",
      "Epoch:  33 \t Current Loss: 1.2163 \t Best Loss: 1.1163 \t Test Loss: 1.2833 \t Fidelity: 0.15127\n",
      "Epoch:  34 \t Current Loss: 1.2166 \t Best Loss: 1.1163 \t Test Loss: 1.2729 \t Fidelity: 0.14656\n",
      "Epoch:  35 \t Current Loss: 1.3659 \t Best Loss: 1.1102 \t Test Loss: 1.2656 \t Fidelity: 0.15163\n",
      "Epoch:  36 \t Current Loss: 1.322 \t Best Loss: 1.1085 \t Test Loss: 1.2567 \t Fidelity: 0.14635\n",
      "Epoch:  37 \t Current Loss: 1.174 \t Best Loss: 1.1085 \t Test Loss: 1.2472 \t Fidelity: 0.15023\n",
      "Epoch:  38 \t Current Loss: 1.3427 \t Best Loss: 1.0751 \t Test Loss: 1.2325 \t Fidelity: 0.14489\n",
      "Epoch:  39 \t Current Loss: 1.2578 \t Best Loss: 1.0751 \t Test Loss: 1.2226 \t Fidelity: 0.14772\n",
      "Epoch:  40 \t Current Loss: 1.2036 \t Best Loss: 1.0751 \t Test Loss: 1.2138 \t Fidelity: 0.14801\n",
      "Epoch:  41 \t Current Loss: 1.1215 \t Best Loss: 1.0751 \t Test Loss: 1.2051 \t Fidelity: 0.14831\n",
      "Epoch:  42 \t Current Loss: 1.1693 \t Best Loss: 1.0514 \t Test Loss: 1.2027 \t Fidelity: 0.15478\n",
      "Epoch:  43 \t Current Loss: 1.1732 \t Best Loss: 0.98853 \t Test Loss: 1.1901 \t Fidelity: 0.14939\n",
      "Epoch:  44 \t Current Loss: 1.2287 \t Best Loss: 0.98853 \t Test Loss: 1.1825 \t Fidelity: 0.14917\n",
      "Epoch:  45 \t Current Loss: 1.1528 \t Best Loss: 0.98853 \t Test Loss: 1.1747 \t Fidelity: 0.15272\n",
      "Epoch:  46 \t Current Loss: 1.0524 \t Best Loss: 0.98853 \t Test Loss: 1.1665 \t Fidelity: 0.14825\n",
      "Epoch:  47 \t Current Loss: 1.1489 \t Best Loss: 0.98853 \t Test Loss: 1.1598 \t Fidelity: 0.15015\n",
      "Epoch:  48 \t Current Loss: 1.1021 \t Best Loss: 0.98853 \t Test Loss: 1.1519 \t Fidelity: 0.14444\n",
      "Epoch:  49 \t Current Loss: 1.0647 \t Best Loss: 0.97601 \t Test Loss: 1.1434 \t Fidelity: 0.14846\n",
      "Epoch:  50 \t Current Loss: 0.97206 \t Best Loss: 0.97099 \t Test Loss: 1.1397 \t Fidelity: 0.15064\n",
      "Epoch:  51 \t Current Loss: 1.1711 \t Best Loss: 0.96285 \t Test Loss: 1.1319 \t Fidelity: 0.1547\n",
      "Epoch:  52 \t Current Loss: 1.1489 \t Best Loss: 0.94995 \t Test Loss: 1.1266 \t Fidelity: 0.15024\n",
      "Epoch:  53 \t Current Loss: 1.1907 \t Best Loss: 0.92391 \t Test Loss: 1.1225 \t Fidelity: 0.15563\n",
      "Epoch:  54 \t Current Loss: 1.1004 \t Best Loss: 0.92391 \t Test Loss: 1.1157 \t Fidelity: 0.1516\n",
      "Epoch:  55 \t Current Loss: 1.104 \t Best Loss: 0.92391 \t Test Loss: 1.11 \t Fidelity: 0.14855\n",
      "Epoch:  56 \t Current Loss: 0.98908 \t Best Loss: 0.92391 \t Test Loss: 1.1059 \t Fidelity: 0.15269\n",
      "Epoch:  57 \t Current Loss: 1.083 \t Best Loss: 0.92391 \t Test Loss: 1.0996 \t Fidelity: 0.15124\n",
      "Epoch:  58 \t Current Loss: 1.0341 \t Best Loss: 0.92391 \t Test Loss: 1.0951 \t Fidelity: 0.1565\n",
      "Epoch:  59 \t Current Loss: 0.99117 \t Best Loss: 0.91624 \t Test Loss: 1.0882 \t Fidelity: 0.14873\n",
      "Epoch:  60 \t Current Loss: 1.1015 \t Best Loss: 0.91624 \t Test Loss: 1.0833 \t Fidelity: 0.14508\n",
      "Epoch:  61 \t Current Loss: 1.0442 \t Best Loss: 0.91624 \t Test Loss: 1.0799 \t Fidelity: 0.1549\n",
      "Epoch:  62 \t Current Loss: 1.0786 \t Best Loss: 0.87954 \t Test Loss: 1.0738 \t Fidelity: 0.14963\n",
      "Epoch:  63 \t Current Loss: 1.0213 \t Best Loss: 0.87954 \t Test Loss: 1.07 \t Fidelity: 0.14839\n",
      "Epoch:  64 \t Current Loss: 0.86203 \t Best Loss: 0.81001 \t Test Loss: 1.0673 \t Fidelity: 0.14589\n",
      "Epoch:  65 \t Current Loss: 0.96666 \t Best Loss: 0.81001 \t Test Loss: 1.0643 \t Fidelity: 0.14647\n",
      "Epoch:  66 \t Current Loss: 0.98853 \t Best Loss: 0.81001 \t Test Loss: 1.0607 \t Fidelity: 0.15009\n",
      "Epoch:  67 \t Current Loss: 0.96739 \t Best Loss: 0.81001 \t Test Loss: 1.0583 \t Fidelity: 0.15789\n",
      "Epoch:  68 \t Current Loss: 1.1081 \t Best Loss: 0.81001 \t Test Loss: 1.054 \t Fidelity: 0.14723\n",
      "Epoch:  69 \t Current Loss: 1.0344 \t Best Loss: 0.81001 \t Test Loss: 1.0512 \t Fidelity: 0.14902\n",
      "Epoch:  70 \t Current Loss: 0.96811 \t Best Loss: 0.81001 \t Test Loss: 1.0487 \t Fidelity: 0.15005\n",
      "Epoch:  71 \t Current Loss: 1.0988 \t Best Loss: 0.81001 \t Test Loss: 1.0458 \t Fidelity: 0.15175\n",
      "Epoch:  72 \t Current Loss: 1.0809 \t Best Loss: 0.81001 \t Test Loss: 1.0438 \t Fidelity: 0.15387\n",
      "Epoch:  73 \t Current Loss: 1.0986 \t Best Loss: 0.81001 \t Test Loss: 1.0412 \t Fidelity: 0.15668\n",
      "Epoch:  74 \t Current Loss: 1.0774 \t Best Loss: 0.81001 \t Test Loss: 1.0393 \t Fidelity: 0.15059\n",
      "Epoch:  75 \t Current Loss: 1.005 \t Best Loss: 0.81001 \t Test Loss: 1.0375 \t Fidelity: 0.15506\n",
      "Epoch:  76 \t Current Loss: 1.0679 \t Best Loss: 0.81001 \t Test Loss: 1.0356 \t Fidelity: 0.1552\n",
      "Epoch:  77 \t Current Loss: 0.99812 \t Best Loss: 0.81001 \t Test Loss: 1.0332 \t Fidelity: 0.15293\n",
      "Epoch:  78 \t Current Loss: 1.0592 \t Best Loss: 0.81001 \t Test Loss: 1.0314 \t Fidelity: 0.15315\n",
      "Epoch:  79 \t Current Loss: 1.1854 \t Best Loss: 0.81001 \t Test Loss: 1.03 \t Fidelity: 0.15306\n",
      "Epoch:  80 \t Current Loss: 0.96157 \t Best Loss: 0.81001 \t Test Loss: 1.0288 \t Fidelity: 0.15118\n",
      "Epoch:  81 \t Current Loss: 1.0311 \t Best Loss: 0.81001 \t Test Loss: 1.0278 \t Fidelity: 0.15393\n",
      "Epoch:  82 \t Current Loss: 0.83075 \t Best Loss: 0.81001 \t Test Loss: 1.0265 \t Fidelity: 0.15692\n",
      "Epoch:  83 \t Current Loss: 0.9936 \t Best Loss: 0.81001 \t Test Loss: 1.0247 \t Fidelity: 0.1545\n",
      "Epoch:  84 \t Current Loss: 1.1044 \t Best Loss: 0.81001 \t Test Loss: 1.0245 \t Fidelity: 0.16171\n",
      "Epoch:  85 \t Current Loss: 1.0103 \t Best Loss: 0.81001 \t Test Loss: 1.0227 \t Fidelity: 0.14897\n",
      "Epoch:  86 \t Current Loss: 1.0092 \t Best Loss: 0.81001 \t Test Loss: 1.0229 \t Fidelity: 0.16093\n",
      "Epoch:  87 \t Current Loss: 1.0466 \t Best Loss: 0.81001 \t Test Loss: 1.0216 \t Fidelity: 0.15314\n",
      "Epoch:  88 \t Current Loss: 1.1363 \t Best Loss: 0.81001 \t Test Loss: 1.0206 \t Fidelity: 0.15459\n",
      "Epoch:  89 \t Current Loss: 1.0438 \t Best Loss: 0.80163 \t Test Loss: 1.0228 \t Fidelity: 0.15389\n",
      "Epoch:  90 \t Current Loss: 0.98218 \t Best Loss: 0.80163 \t Test Loss: 1.0232 \t Fidelity: 0.15624\n",
      "Epoch:  91 \t Current Loss: 0.90047 \t Best Loss: 0.80163 \t Test Loss: 1.0223 \t Fidelity: 0.15803\n",
      "Epoch:  92 \t Current Loss: 1.0505 \t Best Loss: 0.80163 \t Test Loss: 1.0215 \t Fidelity: 0.15763\n",
      "Epoch:  93 \t Current Loss: 1.0013 \t Best Loss: 0.80163 \t Test Loss: 1.0209 \t Fidelity: 0.15478\n",
      "Epoch:  94 \t Current Loss: 1.097 \t Best Loss: 0.80163 \t Test Loss: 1.0208 \t Fidelity: 0.1566\n",
      "Epoch:  95 \t Current Loss: 1.0041 \t Best Loss: 0.80163 \t Test Loss: 1.0198 \t Fidelity: 0.15333\n",
      "Epoch:  96 \t Current Loss: 0.98989 \t Best Loss: 0.80163 \t Test Loss: 1.0195 \t Fidelity: 0.15654\n",
      "Epoch:  97 \t Current Loss: 0.8701 \t Best Loss: 0.80163 \t Test Loss: 1.0193 \t Fidelity: 0.15424\n",
      "Epoch:  98 \t Current Loss: 0.87299 \t Best Loss: 0.80163 \t Test Loss: 1.019 \t Fidelity: 0.15282\n",
      "Epoch:  99 \t Current Loss: 0.94888 \t Best Loss: 0.80163 \t Test Loss: 1.0191 \t Fidelity: 0.1578\n",
      "Epoch: 100 \t Current Loss: 0.95793 \t Best Loss: 0.80163 \t Test Loss: 1.0185 \t Fidelity: 0.1595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainState\n",
       "    model: Chain{@NamedTuple{layer_1::Conv{typeof(σ), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_3::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_4::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_5::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_6::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_7::Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}, layer_8::Dropout{Float64, Colon}, layer_9::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}}, Nothing}((layer_1 = Conv((9,), 1 => 18, σ, pad=4), layer_2 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_3 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_4 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_5 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_6 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_7 = Chain{@NamedTuple{layer_1::Conv{typeof(identity), Int64, Int64, Tuple{Int64}, Tuple{Int64}, Tuple{Int64, Int64}, Tuple{Int64}, Int64, Nothing, Nothing, Static.True, Static.False}, layer_2::BatchNorm{typeof(σ), Float32, Float32, Int64, typeof(zeros32), typeof(ones32), Static.True, Static.True}}, Nothing}((layer_1 = Conv((9,), 18 => 18, pad=4), layer_2 = BatchNorm(18, σ, affine=true, track_stats=true)), nothing), layer_8 = Dropout(0.2), layer_9 = Conv((1,), 18 => 1)), nothing)\n",
       "    # of parameters: 18019\n",
       "    # of states: 224\n",
       "    optimizer: Adam(1.0e-5, (0.9, 0.999), 1.0e-8)\n",
       "    step: 9333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main(tstate::Training.TrainState, epochs)\n",
    "    loss = 1e18\n",
    "    localloss = 100.0\n",
    "    best_tstate = nothing\n",
    "    for epoch in 1:epochs\n",
    "        for (x,y) in train_dataloader\n",
    "#            println(size(x))\n",
    "            _, l, _, tstate = Training.single_train_step!(AutoZygote(), mseloss_function, (x,y), tstate)\n",
    "            localloss = l\n",
    "            if l<loss\n",
    "                loss = l\n",
    "                best_tstate = tstate\n",
    "            end\n",
    "        end\n",
    "        loss_test = mseloss_function(Lux.apply(best_tstate.model, i_test, best_tstate.parameters, best_tstate.states)[1], o_test)\n",
    "        fidelity = cor(vec(Lux.apply(tstate.model, i_test, tstate.parameters, tstate.states)[1]),vec(o_test))\n",
    "        @printf \"Epoch: %3d \\t Current Loss: %.5g \\t Best Loss: %.5g \\t Test Loss: %.5g \\t Fidelity: %.5g\\n\" epoch localloss loss loss_test fidelity\n",
    "    end\n",
    "    return best_tstate\n",
    "end\n",
    "\n",
    "tstate = main(tstate, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b54c03d5-e1d5-4f73-bc26-0b816554e1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22671475f0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor(vec(i_test),vec(o_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0885ee2-11a4-4670-afd3-0922a3b56b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
